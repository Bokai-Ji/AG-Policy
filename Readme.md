# Code of the paper "Learning to Explain: Towards Human-Aligned Interpretability in Deep Reinforcement Learning via Attention Guidance"

You can view the explanation results in `ExplanationVisualization.ipynb`.

You can also simply run `train.py` for training from scratch.

Concept-PPO explanation visualization video of the six ATARI games can be found in `Ours`.

JSM and PSM visualization video can be found in `Jacobian-based` and `Perturbation-based`, respectively.

Questionnaire used in our user study is `user_study.pdf`.

There are also extra explanation results in pdfs in `results/` folder.
