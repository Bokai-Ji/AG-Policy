{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import numpy as np\n",
    "from common.env.atari_wrappers import AttentionGuider\n",
    "from common.model import ConceptNatureModel\n",
    "from common.policy import ConceptPolicy\n",
    "from gymnasium.wrappers import AutoResetWrapper, RecordEpisodeStatistics\n",
    "from gymnasium.wrappers.normalize import RunningMeanStd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizeObservation(gym.Wrapper, gym.utils.RecordConstructorArgs):\n",
    "    \"\"\"This wrapper will normalize observations s.t. each coordinate is centered with unit variance.\n",
    "\n",
    "    Note:\n",
    "        The normalization depends on past trajectories and observations will not be normalized correctly if the wrapper was\n",
    "        newly instantiated or the policy was changed recently.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env: gym.Env, epsilon: float = 1e-8):\n",
    "        \"\"\"This wrapper will normalize observations s.t. each coordinate is centered with unit variance.\n",
    "\n",
    "        Args:\n",
    "            env (Env): The environment to apply the wrapper\n",
    "            epsilon: A stability parameter that is used when scaling the observations.\n",
    "        \"\"\"\n",
    "        gym.utils.RecordConstructorArgs.__init__(self, epsilon=epsilon)\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "\n",
    "        try:\n",
    "            self.num_envs = self.get_wrapper_attr(\"num_envs\")\n",
    "            self.is_vector_env = self.get_wrapper_attr(\"is_vector_env\")\n",
    "        except AttributeError:\n",
    "            self.num_envs = 1\n",
    "            self.is_vector_env = False\n",
    "\n",
    "        if self.is_vector_env:\n",
    "            self.obs_rms = RunningMeanStd(shape=self.single_observation_space.shape)\n",
    "        else:\n",
    "            self.obs_rms = RunningMeanStd(shape=self.observation_space.shape)\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Steps through the environment and normalizes the observation.\"\"\"\n",
    "        obs, rews, terminateds, truncateds, infos = self.env.step(action)\n",
    "        infos['origin_obs'] = obs\n",
    "        if self.is_vector_env:\n",
    "            obs = self.normalize(obs)\n",
    "        else:\n",
    "            obs = self.normalize(np.array([obs]))[0]\n",
    "        return obs, rews, terminateds, truncateds, infos\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        \"\"\"Resets the environment and normalizes the observation.\"\"\"\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "        info['origin_obs'] = obs\n",
    "        if self.is_vector_env:\n",
    "            return self.normalize(obs), info\n",
    "        else:\n",
    "            return self.normalize(np.array([obs]))[0], info\n",
    "\n",
    "    def normalize(self, obs):\n",
    "        \"\"\"Normalises the observation using the running mean and variance of the observations.\"\"\"\n",
    "        self.obs_rms.update(obs)\n",
    "        return (obs - self.obs_rms.mean) / np.sqrt(self.obs_rms.var + self.epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha_blending(original_image, concept_attn, patch_size, alpha, threshold, color_list):\n",
    "    num_concepts = concept_attn.shape[-1]\n",
    "    attention_image = np.copy(original_image) / 255.\n",
    "    num_patches_row = original_image.shape[0] // patch_size\n",
    "    num_patches_col = original_image.shape[1] // patch_size\n",
    "    attention_image *= (1 - alpha)\n",
    "    for c in range(num_concepts):\n",
    "        attention_map = concept_attn[:, c].reshape((num_patches_row, num_patches_col))\n",
    "        attention_map = (attention_map > threshold).astype(float)\n",
    "        attention_map = np.repeat(np.repeat(attention_map, patch_size, axis=0), patch_size, axis=1)\n",
    "        attention_image += alpha * attention_map[:, :, np.newaxis] * color_list[c]\n",
    "    attention_image *= 255\n",
    "    attention_image = attention_image.astype(np.uint8)\n",
    "    \n",
    "    return attention_image\n",
    "\n",
    "def hex_to_rgb(hex):\n",
    "  return tuple(int(hex[i:i+2], 16) for i in (0, 2, 4))\n",
    "\n",
    "def make_envs(n_env, env_id, concept_dict=None, search_area=None, patch_size=None):\n",
    "        def make_env():\n",
    "            env = gym.make(env_id)\n",
    "            env = AttentionGuider(env, concept_dict, search_area, patch_size)\n",
    "            env = AutoResetWrapper(env)\n",
    "            return env\n",
    "        return gym.vector.SyncVectorEnv([make_env for _ in range(n_env)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.num_envs to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.num_envs` for environment variables or `env.get_wrapper_attr('num_envs')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.is_vector_env to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.is_vector_env` for environment variables or `env.get_wrapper_attr('is_vector_env')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.single_observation_space to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.single_observation_space` for environment variables or `env.get_wrapper_attr('single_observation_space')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env_name = 'ALE/Pong-v5'\n",
    "patch_size = 10\n",
    "\n",
    "concept_dict = {\n",
    "    \"player\": np.array([92, 186, 92]),\n",
    "    \"enemy\": np.array([213, 130, 74]),\n",
    "    \"ball\": np.array([236, 236, 236]),\n",
    "}\n",
    "search_area = [[34, 194],[]]\n",
    "env = make_envs(1, env_name, concept_dict, search_area, patch_size)\n",
    "env = NormalizeObservation(env)\n",
    "env = RecordEpisodeStatistics(env)\n",
    "hex_color_list = ['#A52A2A', '#008B8B', '#8FBC8F']\n",
    "color_list = [np.array(hex_to_rgb(hex_color_list[i][1:]))/255. for i in range(len(hex_color_list))]\n",
    "concepts = [\"Player\", \"Enemy\", \"Ball\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluateAgent:\n",
    "    def __init__(self,\n",
    "                 policy,\n",
    "                 device) -> None:\n",
    "        self.policy = policy\n",
    "        self.device = device\n",
    "    \n",
    "    def predict(self, obs, hidden_state, done):\n",
    "        with torch.no_grad():\n",
    "            obs = torch.FloatTensor(obs).to(device=self.device)\n",
    "            hidden_state = torch.FloatTensor(hidden_state).to(device=self.device)\n",
    "            mask = torch.FloatTensor(1 - done).to(device=self.device)\n",
    "            dist, value, mask_attn, concept_attn = self.policy(obs, hidden_state, mask)\n",
    "            act = dist.sample()\n",
    "            log_prob_act = dist.log_prob(act)\n",
    "        return act.cpu().numpy(), log_prob_act.cpu().numpy(), value.cpu().numpy(), hidden_state.cpu().numpy(), concept_attn.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL LOADED\n"
     ]
    }
   ],
   "source": [
    "param_dict = {'num_heads': 2, 'n_concepts': len(concept_dict.keys())}\n",
    "embedder = ConceptNatureModel(in_channels=3, patch_size=patch_size, output_dim=64)\n",
    "policy = ConceptPolicy(embedder=embedder, recurrent=False, action_size=env.action_space[0].n, **param_dict)\n",
    "agent = EvaluateAgent(policy=policy, device='cuda')\n",
    "agent.policy.load_state_dict(torch.load('trained_models\\Pong\\AG_Policy\\model.pth',\n",
    "                             map_location='cuda')['state_dict'])\n",
    "agent.policy.cuda()\n",
    "print('MODEL LOADED')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, info = env.reset()\n",
    "time_step = 0\n",
    "attention_image_list = []\n",
    "concept_attn_list = []\n",
    "hidden_state = np.zeros(256)\n",
    "done = False\n",
    "last_obs = info['origin_obs'][0]\n",
    "while time_step < 200:\n",
    "    act, log_prob_act, value, next_hidden_state, expl_pred = agent.predict(obs, hidden_state, done)\n",
    "    next_obs, reward, done, truncated, info = env.step(act)\n",
    "    attention_image = alpha_blending(\n",
    "        original_image = last_obs,\n",
    "        concept_attn = expl_pred[0],\n",
    "        patch_size=patch_size,\n",
    "        alpha = 0.5,\n",
    "        threshold = 0.5,\n",
    "        color_list = color_list\n",
    "    )\n",
    "    last_obs = info['origin_obs'][0]\n",
    "    attention_image_list.append(attention_image)\n",
    "    concept_attn_list.append(expl_pred[0])\n",
    "    obs = next_obs\n",
    "    time_step += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanations\n",
    "\n",
    "- `#A52A2A` (Red) : Player\n",
    "- `#008B8B` (Blue) : Enemy\n",
    "- `#8FBC8F` (Green) : Ball"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(dpi=100)\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    plt.imshow(attention_image_list[50+i])\n",
    "    plt.axis('off')\n",
    "plt.subplots_adjust(wspace=0.05, hspace=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Marginal Distribution\n",
    "\n",
    "- `#A52A2A` (Red) : Player\n",
    "- `#008B8B` (Blue) : Enemy\n",
    "- `#8FBC8F` (Green) : Ball\n",
    "\n",
    "The distribution of attention weights on each head for Pong frame. The two stacked bar plots show the sum of the weights along the x and y axis (the range of each plot is [0, 1].)\n",
    "\n",
    "You can feel free to modify `FRAME` below to see results in other frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.3, 16.3, 0.0, 2.9642510414123535)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "FRAME = 25\n",
    "x_list = []\n",
    "y_list = []\n",
    "fig = plt.figure(dpi=100)\n",
    "gs = GridSpec(260,210)\n",
    "time_step += 1\n",
    "ax_joint = fig.add_subplot(gs[21:231,0:160])\n",
    "ax_marg_x = fig.add_subplot(gs[0:20,32:128])\n",
    "ax_marg_y = fig.add_subplot(gs[21:231,123:138])\n",
    "ax_joint.imshow(attention_image_list[FRAME])\n",
    "bottom_x = np.zeros(16)\n",
    "bottom_y = np.zeros(21)\n",
    "for idx in range(len(concept_dict)):\n",
    "    concept_map = concept_attn_list[FRAME][:, idx].reshape((attention_image.shape[0] // patch_size, attention_image.shape[1] // patch_size))\n",
    "    concept_map = np.where(concept_map>0.5, concept_map, 0.0)\n",
    "    x_list.append(concept_map.sum(axis=0))\n",
    "    y_list.append(concept_map.sum(axis=1))\n",
    "for idx in range(len(concept_dict)):\n",
    "    x = x_list[idx]\n",
    "    y = y_list[idx]\n",
    "    ax_marg_x.bar(x = np.arange(0, len(x)), height=x, alpha=1, width=1, color = hex_color_list[idx], bottom=bottom_x)\n",
    "    ax_marg_y.barh(y = np.arange(len(y), 0, -1), height=y, alpha=1, width=1, color = hex_color_list[idx], left=bottom_y)\n",
    "    bottom_x = x_list[idx] + bottom_x\n",
    "    bottom_y = y_list[idx] + bottom_y\n",
    "\n",
    "# Turn off tick labels on marginals\n",
    "plt.setp(ax_marg_x.get_xticklabels(), visible=False)\n",
    "plt.setp(ax_marg_y.get_yticklabels(), visible=False)\n",
    "\n",
    "# Turn off axis\n",
    "ax_joint.axis('off')\n",
    "ax_marg_y.axis('off')\n",
    "ax_marg_x.axis('off')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
